{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing steps. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "def pre_process(msg):\n",
    "    msg = str(msg)\n",
    "    msg = msg.lower()\n",
    "    msg = re.sub('[^a-zA-Z]',' ', msg)\n",
    "    msg = nltk.word_tokenize(msg)\n",
    "    msg = [wordnet.lemmatize(word) for word in msg if word not in stop_words]\n",
    "    msg = ' '.join(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Training_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Sentiment (1 for positive and 0 for negative)</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Features</th>\n",
       "      <th>Purchase/interaction experience (delivery/packaging, customer care etc)</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ansh</td>\n",
       "      <td>audio</td>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>Shekhar Kumar</td>\n",
       "      <td>Amazing earphones with value for money</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing earphones with value for money</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ansh</td>\n",
       "      <td>audio</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>Pravin velumani</td>\n",
       "      <td>worst quality</td>\n",
       "      <td>2</td>\n",
       "      <td>in the beginning the sound quantity, bass are ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ansh</td>\n",
       "      <td>audio</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>Soubhagya Biswal</td>\n",
       "      <td>Best buy in the budget</td>\n",
       "      <td>5</td>\n",
       "      <td>Using it for more than a month. Sound quality ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ansh</td>\n",
       "      <td>audio</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>Soubhagya Biswal</td>\n",
       "      <td>Best buy in the budget</td>\n",
       "      <td>5</td>\n",
       "      <td>It has magnetic hold system also. Cable is ver...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ansh</td>\n",
       "      <td>audio</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>Soubhagya Biswal</td>\n",
       "      <td>Best buy in the budget</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5mm jack is solid to take heavy strain. All ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name category       date            author  \\\n",
       "0  Ansh    audio 2019-03-14     Shekhar Kumar   \n",
       "1  Ansh    audio 2019-05-29   Pravin velumani   \n",
       "2  Ansh    audio 2019-09-11  Soubhagya Biswal   \n",
       "3  Ansh    audio 2019-09-11  Soubhagya Biswal   \n",
       "4  Ansh    audio 2019-09-11  Soubhagya Biswal   \n",
       "\n",
       "                                    title  rating  \\\n",
       "0  Amazing earphones with value for money       5   \n",
       "1                           worst quality       2   \n",
       "2                  Best buy in the budget       5   \n",
       "3                  Best buy in the budget       5   \n",
       "4                  Best buy in the budget       5   \n",
       "\n",
       "                                            sentence  \\\n",
       "0             Amazing earphones with value for money   \n",
       "1  in the beginning the sound quantity, bass are ...   \n",
       "2  Using it for more than a month. Sound quality ...   \n",
       "3  It has magnetic hold system also. Cable is ver...   \n",
       "4  3.5mm jack is solid to take heavy strain. All ...   \n",
       "\n",
       "   Sentiment (1 for positive and 0 for negative) Quality  Features  \\\n",
       "0                                            1.0     NaN       NaN   \n",
       "1                                            0.0     NaN       NaN   \n",
       "2                                            1.0       1       NaN   \n",
       "3                                            1.0     NaN       1.0   \n",
       "4                                            1.0     NaN       1.0   \n",
       "\n",
       "   Purchase/interaction experience (delivery/packaging, customer care etc)  \\\n",
       "0                                                NaN                         \n",
       "1                                                1.0                         \n",
       "2                                                NaN                         \n",
       "3                                                NaN                         \n",
       "4                                                NaN                         \n",
       "\n",
       "   Price  Rand  \n",
       "0    1.0  88.0  \n",
       "1    NaN  89.0  \n",
       "2    NaN  84.0  \n",
       "3    NaN  89.0  \n",
       "4    NaN  87.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'category', 'date', 'author', 'title', 'rating', 'sentence',\n",
       "       'Sentiment (1 for positive and 0 for negative)', 'Quality', 'Features',\n",
       "       'Purchase/interaction experience (delivery/packaging, customer care etc)',\n",
       "       'Price', 'Rand'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['title','sentence','Quality','Features','Purchase/interaction experience (delivery/packaging, customer care etc)','Price']]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Quality'] = pd.to_numeric(df_new['Quality'],errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11174 entries, 0 to 11173\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                                                   Non-Null Count  Dtype  \n",
      "---  ------                                                                   --------------  -----  \n",
      " 0   title                                                                    11174 non-null  object \n",
      " 1   sentence                                                                 11174 non-null  object \n",
      " 2   Quality                                                                  2742 non-null   float64\n",
      " 3   Features                                                                 3465 non-null   float64\n",
      " 4   Purchase/interaction experience (delivery/packaging, customer care etc)  2824 non-null   float64\n",
      " 5   Price                                                                    1425 non-null   float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 523.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill na values with zero \n",
    "df_new.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to detect rows that don't have any classification labels\n",
    "#For that we will take sum of all the classes, if the sum is zero, that row has no classes and can be removed\n",
    "def na_class(df):\n",
    "    idx_lst = []\n",
    "    for i in range(0,len(df)):\n",
    "        sum = df['Quality'].iloc[i] + df['Features'].iloc[i] + df['Purchase/interaction experience (delivery/packaging, customer care etc)'].iloc[i] + df['Price'].iloc[i]\n",
    "        if sum == 0 :\n",
    "            idx_lst.append(i)\n",
    "    return idx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "idx_lst = na_class(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with no class:11.365670306067658\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of rows with no class:{}\".format(len(idx_lst)/len(df_new)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop these rows\n",
    "df_new.drop(idx_lst, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create tokens \n",
    "def create_tokens(doc):\n",
    "    doc = nlp(doc)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokens = list(dict.fromkeys(tokens))\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pre_process and get tokens for all the text files\n",
    "def process(df):\n",
    "    df_new1 = pd.DataFrame()\n",
    "    df_new1['title'] = df['title'].apply(lambda x: pre_process(x))\n",
    "    df_new1['sentence'] = df['sentence'].apply(lambda x: pre_process(x))\n",
    "    text = df_new1['title'] + ' ' + df_new1['sentence']\n",
    "    document = list()\n",
    "    for i in range(0,len(text)):\n",
    "        tokens = create_tokens(text.iloc[i])\n",
    "        document.append(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = process(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = df_new[['Quality','Features','Purchase/interaction experience (delivery/packaging, customer care etc)','Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X_data,Y_data,test_size = 0.25,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 66\n",
      "Vocabulary size: 4700\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(x_train)\n",
    "# calculate max document length\n",
    "length = max_length(x_train)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7428, 66)\n"
     ]
    }
   ],
   "source": [
    "# encode data\n",
    "trainX = encode_text(tokenizer, x_train, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(4, activation='softmax')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 66)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 66)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 66)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 66, 100)      470000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 66, 100)      470000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 66, 100)      470000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 63, 32)       12832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 61, 32)       19232       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 59, 32)       25632       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 63, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 61, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 59, 32)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 31, 32)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 30, 32)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 29, 32)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 992)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 960)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 928)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2880)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           28810       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            44          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,496,550\n",
      "Trainable params: 86,550\n",
      "Non-trainable params: 1,410,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 1.1024 - accuracy: 0.5761\n",
      "Epoch 2/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.8801 - accuracy: 0.6865\n",
      "Epoch 3/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.7929 - accuracy: 0.7202\n",
      "Epoch 4/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.7186 - accuracy: 0.7583\n",
      "Epoch 5/50\n",
      "7428/7428 [==============================] - 17s 2ms/step - loss: 0.6656 - accuracy: 0.7756\n",
      "Epoch 6/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.6196 - accuracy: 0.7942\n",
      "Epoch 7/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.5759 - accuracy: 0.8096\n",
      "Epoch 8/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.5380 - accuracy: 0.8250\n",
      "Epoch 9/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.5201 - accuracy: 0.8349\n",
      "Epoch 10/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.4788 - accuracy: 0.8492\n",
      "Epoch 11/50\n",
      "7428/7428 [==============================] - 16s 2ms/step - loss: 0.4673 - accuracy: 0.8511\n",
      "Epoch 12/50\n",
      "7428/7428 [==============================] - 16s 2ms/step - loss: 0.4597 - accuracy: 0.8574\n",
      "Epoch 13/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.4405 - accuracy: 0.8601\n",
      "Epoch 14/50\n",
      "7428/7428 [==============================] - 16s 2ms/step - loss: 0.4249 - accuracy: 0.8681\n",
      "Epoch 15/50\n",
      "7428/7428 [==============================] - 16s 2ms/step - loss: 0.4135 - accuracy: 0.8730\n",
      "Epoch 16/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3989 - accuracy: 0.8792\n",
      "Epoch 17/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3816 - accuracy: 0.8798\n",
      "Epoch 18/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3874 - accuracy: 0.8856\n",
      "Epoch 19/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3786 - accuracy: 0.8821\n",
      "Epoch 20/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3700 - accuracy: 0.8883 1s - loss: 0.3678 - ac - ETA: 0s - loss:\n",
      "Epoch 21/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3623 - accuracy: 0.8887 0s - loss: 0.3617 - accuracy\n",
      "Epoch 22/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3550 - accuracy: 0.8904 0s - loss: 0.3533 - accuracy: 0.89 - ETA: 0s - loss: 0.3533 - accu - ETA: 0s - loss: 0.3522 - \n",
      "Epoch 23/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3512 - accuracy: 0.8924\n",
      "Epoch 24/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3313 - accuracy: 0.9019 0s - loss: 0.3307 - accuracy\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3379 - accuracy: 0.8997\n",
      "Epoch 26/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3217 - accuracy: 0.9023\n",
      "Epoch 27/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3277 - accuracy: 0.9002 0s - loss: 0\n",
      "Epoch 28/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3228 - accuracy: 0.9046\n",
      "Epoch 29/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3132 - accuracy: 0.9047 0s\n",
      "Epoch 30/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3026 - accuracy: 0.9116E\n",
      "Epoch 31/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3138 - accuracy: 0.9044 0s - l\n",
      "Epoch 32/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3066 - accuracy: 0.9064 0s - loss: 0.3031 \n",
      "Epoch 33/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3059 - accuracy: 0.9074 2s - loss:\n",
      "Epoch 34/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.3035 - accuracy: 0.9118 0s - loss: 0.3041 - accuracy\n",
      "Epoch 35/50\n",
      "7428/7428 [==============================] - 16s 2ms/step - loss: 0.3013 - accuracy: 0.9087 0s - l\n",
      "Epoch 36/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2898 - accuracy: 0.9113 1s - loss: 0.2 - ETA: 0s - loss: 0.2896 - \n",
      "Epoch 37/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2822 - accuracy: 0.9187\n",
      "Epoch 38/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2804 - accuracy: 0.9151\n",
      "Epoch 39/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2879 - accuracy: 0.9142\n",
      "Epoch 40/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2823 - accuracy: 0.9179 1s - loss: 0.2 - ETA\n",
      "Epoch 41/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2809 - accuracy: 0.9124 \n",
      "Epoch 42/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2776 - accuracy: 0.9156\n",
      "Epoch 43/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2828 - accuracy: 0.9149\n",
      "Epoch 44/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2762 - accuracy: 0.9183\n",
      "Epoch 45/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2750 - accuracy: 0.9184\n",
      "Epoch 46/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2692 - accuracy: 0.9168\n",
      "Epoch 47/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2704 - accuracy: 0.9212 0s - loss:\n",
      "Epoch 48/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2610 - accuracy: 0.9238 0s - loss: 0.2611 - accuracy\n",
      "Epoch 49/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2501 - accuracy: 0.9258\n",
      "Epoch 50/50\n",
      "7428/7428 [==============================] - 15s 2ms/step - loss: 0.2610 - accuracy: 0.9206 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x232214f1bc8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], y_train.values, epochs=50, batch_size=10)\n",
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2476, 66)\n"
     ]
    }
   ],
   "source": [
    "testX = encode_text(tokenizer, x_test, length)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict([testX,testX,testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predf = []\n",
    "for i in y_preds:\n",
    "    lst1 = [] \n",
    "    for j in i:\n",
    "        if j > 0.5:\n",
    "            j =1 \n",
    "        else:\n",
    "            j=0\n",
    "        lst1.append(j)\n",
    "    y_predf.append(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.63      0.70       679\n",
      "           1       0.75      0.71      0.73       889\n",
      "           2       0.69      0.65      0.67       686\n",
      "           3       0.77      0.72      0.75       353\n",
      "\n",
      "   micro avg       0.74      0.67      0.71      2607\n",
      "   macro avg       0.75      0.68      0.71      2607\n",
      "weighted avg       0.74      0.67      0.71      2607\n",
      " samples avg       0.71      0.69      0.69      2607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
