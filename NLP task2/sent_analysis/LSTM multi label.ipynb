{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing steps. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "def pre_process(msg):\n",
    "    msg = str(msg)\n",
    "    msg = msg.lower()\n",
    "    msg = re.sub('[^a-zA-Z]',' ', msg)\n",
    "    msg = nltk.word_tokenize(msg)\n",
    "    msg = [wordnet.lemmatize(word) for word in msg if word not in stop_words]\n",
    "    msg = ' '.join(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Training_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['title','sentence','Quality','Features','Purchase/interaction experience (delivery/packaging, customer care etc)','Price']]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Quality'] = pd.to_numeric(df_new['Quality'],errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill na values with zero \n",
    "df_new.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to detect rows that don't have any classification labels\n",
    "#For that we will take sum of all the classes, if the sum is zero, that row has no classes and can be removed\n",
    "def na_class(df):\n",
    "    idx_lst = []\n",
    "    for i in range(0,len(df)):\n",
    "        sum = df['Quality'].iloc[i] + df['Features'].iloc[i] + df['Purchase/interaction experience (delivery/packaging, customer care etc)'].iloc[i] + df['Price'].iloc[i]\n",
    "        if sum == 0 :\n",
    "            idx_lst.append(i)\n",
    "    return idx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "idx_lst = na_class(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop these rows\n",
    "df_new.drop(idx_lst, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df_new1 = pd.DataFrame()\n",
    "    df_new1['title'] = df['title'].apply(lambda x: pre_process(x))\n",
    "    df_new1['sentence'] = df['sentence'].apply(lambda x: pre_process(x))\n",
    "    text = df_new1['title'] + ' ' + df_new1['sentence']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create tokens \n",
    "def create_tokens(doc):\n",
    "    doc = nlp(doc)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokens = list(dict.fromkeys(tokens))\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pre_process and get tokens for all the text files\n",
    "def process(df):\n",
    "    df_new1 = pd.DataFrame()\n",
    "    df_new1['title'] = df['title'].apply(lambda x: pre_process(x))\n",
    "    df_new1['sentence'] = df['sentence'].apply(lambda x: pre_process(x))\n",
    "    text = df_new1['title'] + ' ' + df_new1['sentence']\n",
    "    document = list()\n",
    "    for i in range(0,len(text)):\n",
    "        tokens = create_tokens(text.iloc[i])\n",
    "        document.append(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = process(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = df_new[['Quality','Features','Purchase/interaction experience (delivery/packaging, customer care etc)','Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(4, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 100)          481500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 599,264\n",
      "Trainable params: 117,764\n",
      "Non-trainable params: 481,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\No_ob0dy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6338 samples, validate on 1585 samples\n",
      "Epoch 1/20\n",
      "6338/6338 [==============================] - 101s 16ms/step - loss: 0.5668 - acc: 0.7353 - val_loss: 0.5671 - val_acc: 0.7377\n",
      "Epoch 2/20\n",
      "6338/6338 [==============================] - 123s 19ms/step - loss: 0.5637 - acc: 0.7354 - val_loss: 0.5670 - val_acc: 0.7377\n",
      "Epoch 3/20\n",
      "6338/6338 [==============================] - 126s 20ms/step - loss: 0.5636 - acc: 0.7354 - val_loss: 0.5632 - val_acc: 0.7377\n",
      "Epoch 4/20\n",
      "6338/6338 [==============================] - 131s 21ms/step - loss: 0.5633 - acc: 0.7354 - val_loss: 0.5621 - val_acc: 0.7377\n",
      "Epoch 5/20\n",
      "6338/6338 [==============================] - 133s 21ms/step - loss: 0.5632 - acc: 0.7354 - val_loss: 0.5635 - val_acc: 0.7377\n",
      "Epoch 6/20\n",
      "6338/6338 [==============================] - 136s 21ms/step - loss: 0.5629 - acc: 0.7354 - val_loss: 0.5631 - val_acc: 0.7377\n",
      "Epoch 7/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5631 - acc: 0.7354 - val_loss: 0.5621 - val_acc: 0.7377\n",
      "Epoch 8/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5629 - acc: 0.7354 - val_loss: 0.5625 - val_acc: 0.7377\n",
      "Epoch 9/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5625 - val_acc: 0.7377\n",
      "Epoch 10/20\n",
      "6338/6338 [==============================] - 145s 23ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5620 - val_acc: 0.7377\n",
      "Epoch 11/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5628 - acc: 0.7354 - val_loss: 0.5625 - val_acc: 0.7377\n",
      "Epoch 12/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5628 - acc: 0.7354 - val_loss: 0.5619 - val_acc: 0.7377\n",
      "Epoch 13/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5628 - acc: 0.7354 - val_loss: 0.5630 - val_acc: 0.7377\n",
      "Epoch 14/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5625 - val_acc: 0.7377\n",
      "Epoch 15/20\n",
      "6338/6338 [==============================] - 138s 22ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5620 - val_acc: 0.7377\n",
      "Epoch 16/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5621 - val_acc: 0.7377\n",
      "Epoch 17/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5626 - acc: 0.7354 - val_loss: 0.5622 - val_acc: 0.7377\n",
      "Epoch 18/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5626 - acc: 0.7354 - val_loss: 0.5625 - val_acc: 0.7377\n",
      "Epoch 19/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5627 - acc: 0.7354 - val_loss: 0.5619 - val_acc: 0.7377\n",
      "Epoch 20/20\n",
      "6338/6338 [==============================] - 137s 22ms/step - loss: 0.5625 - acc: 0.7354 - val_loss: 0.5623 - val_acc: 0.7377\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
