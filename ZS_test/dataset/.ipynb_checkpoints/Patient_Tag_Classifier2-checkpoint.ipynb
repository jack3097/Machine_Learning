{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the train and test data set\n",
    "df_train = pd.read_csv('train.csv',encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv('test.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1157 entries, 0 to 1156\n",
      "Data columns (total 9 columns):\n",
      "Source             1157 non-null object\n",
      "Host               1098 non-null object\n",
      "Link               1157 non-null object\n",
      "Date(ET)           1157 non-null object\n",
      "Time(ET)           1157 non-null object\n",
      "time(GMT)          996 non-null object\n",
      "Title              941 non-null object\n",
      "TRANS_CONV_TEXT    1156 non-null object\n",
      "Patient_Tag        1157 non-null int64\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 81.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>time(GMT)</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "      <th>Patient_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>cafepharma.com</td>\n",
       "      <td>http://cafepharma.com/boards/threads/epstein.5...</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>13:58:00</td>\n",
       "      <td>6/15/2016 23:28</td>\n",
       "      <td>Epstein</td>\n",
       "      <td>I don't disagree with you in principle. I'm ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.patient.co.uk</td>\n",
       "      <td>http://www.patient.co.uk/forums/discuss/enlarg...</td>\n",
       "      <td>5/7/2016</td>\n",
       "      <td>0.820833333</td>\n",
       "      <td>42498.21667</td>\n",
       "      <td>Enlarged Heart.Thread Enlarged Heart</td>\n",
       "      <td>I am always dizzy I get dizzy standing up so I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://abcnewsradioonline.com/entertainment-news</td>\n",
       "      <td>http://abcnewsradioonline.com/entertainment-ne...</td>\n",
       "      <td>4/14/2016</td>\n",
       "      <td>15:00:38</td>\n",
       "      <td>4/15/2016 0:30</td>\n",
       "      <td>Queen Latifah Joins American Heart Association...</td>\n",
       "      <td>Axelle/Bauer-Griffin/FilmMagic(NEW YORK) -- Qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.cancer-forums.net</td>\n",
       "      <td>http://www.cancer-forums.net/viewtopic.php?f=1...</td>\n",
       "      <td>6/18/2016</td>\n",
       "      <td>20:46:00</td>\n",
       "      <td>6/19/2016 6:16</td>\n",
       "      <td>Bulaemia</td>\n",
       "      <td>I am 17 and I have been throwing up for about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.diyaudio.com</td>\n",
       "      <td>http://www.diyaudio.com/forums/lounge/292252-d...</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>3:26:00</td>\n",
       "      <td>6/15/2016 12:56</td>\n",
       "      <td>DIY Silver interconnects and RCAs???</td>\n",
       "      <td>Quote: Originally Posted by Boyan Silyavski Wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                                              Host  \\\n",
       "0  FORUMS                                    cafepharma.com   \n",
       "1  FORUMS                                 www.patient.co.uk   \n",
       "2    BLOG  http://abcnewsradioonline.com/entertainment-news   \n",
       "3  FORUMS                             www.cancer-forums.net   \n",
       "4  FORUMS                                  www.diyaudio.com   \n",
       "\n",
       "                                                Link   Date(ET)     Time(ET)  \\\n",
       "0  http://cafepharma.com/boards/threads/epstein.5...  6/15/2016     13:58:00   \n",
       "1  http://www.patient.co.uk/forums/discuss/enlarg...   5/7/2016  0.820833333   \n",
       "2  http://abcnewsradioonline.com/entertainment-ne...  4/14/2016     15:00:38   \n",
       "3  http://www.cancer-forums.net/viewtopic.php?f=1...  6/18/2016     20:46:00   \n",
       "4  http://www.diyaudio.com/forums/lounge/292252-d...  6/15/2016      3:26:00   \n",
       "\n",
       "         time(GMT)                                              Title  \\\n",
       "0  6/15/2016 23:28                                            Epstein   \n",
       "1      42498.21667               Enlarged Heart.Thread Enlarged Heart   \n",
       "2   4/15/2016 0:30  Queen Latifah Joins American Heart Association...   \n",
       "3   6/19/2016 6:16                                           Bulaemia   \n",
       "4  6/15/2016 12:56               DIY Silver interconnects and RCAs???   \n",
       "\n",
       "                                     TRANS_CONV_TEXT  Patient_Tag  \n",
       "0  I don't disagree with you in principle. I'm ju...            0  \n",
       "1  I am always dizzy I get dizzy standing up so I...            1  \n",
       "2  Axelle/Bauer-Griffin/FilmMagic(NEW YORK) -- Qu...            0  \n",
       "3  I am 17 and I have been throwing up for about ...            1  \n",
       "4  Quote: Originally Posted by Boyan Silyavski Wa...            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are null values present in the columns host and title. We will fill them with datas extracted from the link\n",
    "#But before that we will drop the row for which the conversation is empty\n",
    "df_train.drop(df_train[df_train['TRANS_CONV_TEXT'].isna()].index[0],inplace=True)\n",
    "df_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract week of the day,month, hour and from date and time column\n",
    "def datetime_data(df):\n",
    "    #First we convert to datetime format\n",
    "    df['Date(ET)'] = df['Date(ET)'].apply(lambda x: pd.to_datetime(x,errors = 'coerce',format='%m/%d/%Y'))\n",
    "    df['Time(ET)'] = df['Time(ET)'].apply(lambda x: pd.to_datetime(x,errors = 'coerce'))\n",
    "    df['Weekday'] = df['Date(ET)'].apply(lambda x: x.weekday())\n",
    "    df['Month'] = df['Date(ET)'].apply(lambda x: x.month)\n",
    "    df['Hour'] = df['Time(ET)'].apply(lambda x: x.hour)\n",
    "    #Now we can drop the date(et) and time(et) columns\n",
    "    df.drop(['Date(ET)','Time(ET)'],axis=1,inplace=True)\n",
    "    #Now let's fill the NA values in our generated column with mode\n",
    "    df['Hour'].fillna(df['Hour'].mode().iloc[0],inplace=True)\n",
    "    df['Weekday'].fillna(df['Weekday'].mode().iloc[0],inplace=True)\n",
    "    df['Month'].fillna(df['Month'].mode().iloc[0],inplace=True)\n",
    "    #Now we convert them to string type for dummy creation\n",
    "    df['Hour'] = df['Hour'].astype(str)\n",
    "    df['Weekday'] = df['Weekday'].astype(str)\n",
    "    df['Month'] = df['Month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract hostname from url\n",
    "def get_url(x):\n",
    "    y = urlparse(x)\n",
    "    return(y.netloc)\n",
    "\n",
    "#function to extract title from url\n",
    "def get_title(x):\n",
    "    y = urlparse(x)\n",
    "    y = y.path\n",
    "    y = y.split(sep='/')\n",
    "    return(y[-1])\n",
    "#funtion to fill na values in Title\n",
    "def fillna_title(df):\n",
    "    lst = df[df['Title'].isna()].index.tolist()\n",
    "    for i in lst:\n",
    "        df['Title'].iloc[i] = get_title(df['Link'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to wrap above funcs and return a clean data set\n",
    "def clean_data(df):\n",
    "    #First we drop columns that are not needed\n",
    "    df.drop(['Host','time(GMT)'],axis=1,inplace=True)\n",
    "    #make a column for host\n",
    "    df['Host'] = df['Link'].apply(lambda x: get_url(x))\n",
    "    #Call function to fill na values in title\n",
    "    fillna_title(df)\n",
    "    #Process date time data\n",
    "    datetime_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\No_ob0dy\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Then we call the function to clean_data\n",
    "clean_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1156 entries, 0 to 1155\n",
      "Data columns (total 9 columns):\n",
      "Source             1156 non-null object\n",
      "Link               1156 non-null object\n",
      "Title              1156 non-null object\n",
      "TRANS_CONV_TEXT    1156 non-null object\n",
      "Patient_Tag        1156 non-null int64\n",
      "Host               1156 non-null object\n",
      "Weekday            1156 non-null object\n",
      "Month              1156 non-null object\n",
      "Hour               1156 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 81.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to pre process for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer(max_features=13000)\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['www','co','com','org','net','uk','blogspot','info','de'])\n",
    "def pre_process(msg,corpus):\n",
    "    msg = msg.lower()\n",
    "    msg = re.sub('[^a-zA-Z]',' ', msg)\n",
    "    msg = nltk.word_tokenize(msg)\n",
    "    msg = [wordnet.lemmatize(word) for word in msg if word not in stopwords]\n",
    "    msg = ' '.join(msg)\n",
    "    corpus.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pre-process\n",
    "def pre_process_train(df):\n",
    "    train = pd.DataFrame()\n",
    "    train['Text'] = df[['Host','Title','TRANS_CONV_TEXT']].apply(lambda x: ' '.join(x),axis=1)\n",
    "    #train = pd.concat([train1,df[['Source']]],axis=1)\n",
    "    #del train1\n",
    "    \n",
    "    corpus = []\n",
    "    train['Text'].apply(lambda x : pre_process(x,corpus))\n",
    "    \n",
    "    tfidf.fit(corpus)\n",
    "    x = tfidf.transform(corpus).toarray()\n",
    "    x1 = pd.DataFrame(x)\n",
    "    \n",
    "    #splitting the categorical variables and numerical variables\n",
    "    x_cat = df[['Source','Weekday','Month','Hour']]\n",
    "    train_dums = pd.get_dummies(x_cat,drop_first=True)\n",
    "    del train\n",
    "    \n",
    "    train = pd.concat([train_dums,x1],axis=1)\n",
    "    del x1, train_dums, x, x_cat\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pre_process_train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source_FACEBOOK</th>\n",
       "      <th>Source_FORUMS</th>\n",
       "      <th>Source_Facebook</th>\n",
       "      <th>Source_YOUTUBE</th>\n",
       "      <th>Weekday_1.0</th>\n",
       "      <th>Weekday_2.0</th>\n",
       "      <th>Weekday_3.0</th>\n",
       "      <th>Weekday_4.0</th>\n",
       "      <th>Weekday_5.0</th>\n",
       "      <th>Weekday_6.0</th>\n",
       "      <th>...</th>\n",
       "      <th>12990</th>\n",
       "      <th>12991</th>\n",
       "      <th>12992</th>\n",
       "      <th>12993</th>\n",
       "      <th>12994</th>\n",
       "      <th>12995</th>\n",
       "      <th>12996</th>\n",
       "      <th>12997</th>\n",
       "      <th>12998</th>\n",
       "      <th>12999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13038 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source_FACEBOOK  Source_FORUMS  Source_Facebook  Source_YOUTUBE  \\\n",
       "0                0              1                0               0   \n",
       "1                0              1                0               0   \n",
       "2                0              0                0               0   \n",
       "3                0              1                0               0   \n",
       "4                0              1                0               0   \n",
       "\n",
       "   Weekday_1.0  Weekday_2.0  Weekday_3.0  Weekday_4.0  Weekday_5.0  \\\n",
       "0            0            1            0            0            0   \n",
       "1            0            0            0            0            1   \n",
       "2            0            0            1            0            0   \n",
       "3            0            0            0            0            1   \n",
       "4            0            1            0            0            0   \n",
       "\n",
       "   Weekday_6.0  ...  12990  12991  12992  12993  12994  12995  12996  12997  \\\n",
       "0            0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1            0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2            0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3            0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4            0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   12998  12999  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "\n",
       "[5 rows x 13038 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train[['Patient_Tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the function to pre-process test data\n",
    "def pre_process_test(df):\n",
    "    train = pd.DataFrame()\n",
    "    train['Text'] = df[['Host','Title','TRANS_CONV_TEXT']].apply(lambda x: ' '.join(x),axis=1)\n",
    "    #train = pd.concat([train1,df[['Source']]],axis=1)\n",
    "    #del train1\n",
    "    \n",
    "    corpus = []\n",
    "    train['Text'].apply(lambda x : pre_process(x,corpus))\n",
    "    \n",
    "    #tfidf.fit(corpus)\n",
    "    x = tfidf.transform(corpus).toarray()\n",
    "    x1 = pd.DataFrame(x)\n",
    "    \n",
    "    #splitting the categorical variables and numerical variables\n",
    "    x_cat = df[['Source','Weekday','Month','Hour']]\n",
    "    train_dums = pd.get_dummies(x_cat,drop_first=True)\n",
    "    del train\n",
    "    \n",
    "    train = pd.concat([train_dums,x1],axis=1)\n",
    "    del x1, train_dums, x, x_cat\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    916\n",
       "1    240\n",
       "Name: Patient_Tag, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['Patient_Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\No_ob0dy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Above code shows that our data is imbalanced, so we apply synthetic oversampling\n",
    "sm = SMOTE(random_state = 1)\n",
    "train, target = sm.fit_sample(train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1282, 13038)\n",
      "y_train shape: (1282,)\n",
      "x_test shape: (550, 13038)\n",
      "y_test shape: (550,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train,target,test_size=.30,random_state=1)\n",
    "print(\"x_train shape:\",x_train.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"x_test shape:\",x_test.shape)\n",
    "print(\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 500, 700, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "grid_params = {'n_estimators': [200,500,700,1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "rfc1 = GridSearchCV(rfc,grid_params,cv = 10)\n",
    "rfc1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 1000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[271,   8],\n",
       "       [  9, 262]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "cnf2 = confusion_matrix(y_test,y_pred)\n",
    "cnf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       279\n",
      "           1       0.97      0.97      0.97       271\n",
      "\n",
      "    accuracy                           0.97       550\n",
      "   macro avg       0.97      0.97      0.97       550\n",
      "weighted avg       0.97      0.97      0.97       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df,predictor):\n",
    "    #first we clean the test data\n",
    "    clean_data(df)\n",
    "    df.drop('Unnamed: 9',axis=1,inplace=True)\n",
    "    #Now we pre-process the test set\n",
    "    test = pre_process_test(df)\n",
    "    #Now we predict\n",
    "    test_pred = predictor.predict(test)\n",
    "    preds = pd.DataFrame(test_pred,columns=['Patient_Tag'])\n",
    "    preds.reset_index(inplace=True)\n",
    "    preds.rename(columns={'index':'Index'},inplace=True)\n",
    "    preds['Index'] = preds['Index']+1\n",
    "    preds.head()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\No_ob0dy\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "preds = predict(df_test,rfc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv('Submission4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
